# VAE Cheat Sheet (One-Page Reference)

## Core Equations

### ELBO (Evidence Lower Bound)
```
ð“›(Î¸,Ï†;x) = ð”¼_q[log p_Î¸(x|z)] - D_KL(q_Ï†(z|x) â€– p(z))
          = Reconstruction    -  Regularization
```

### Fundamental Inequality
```
log p_Î¸(x) â‰¥ ð“›(Î¸,Ï†;x)
log p_Î¸(x) = ð“›(Î¸,Ï†;x) + D_KL(q_Ï†(z|x) â€– p_Î¸(z|x))
```

### Reparameterization Trick
```
z = Î¼ + Ïƒ âŠ™ Îµ,  where Îµ ~ ð’©(0, I)
```

### KL Divergence (Gaussian to Standard Normal)
```
D_KL(ð’©(Î¼,ÏƒÂ²) â€– ð’©(0,I)) = Â½ Î£â±¼(Î¼â±¼Â² + Ïƒâ±¼Â² - log Ïƒâ±¼Â² - 1)
```

---

## PyTorch Code Snippets

### Reparameterization
```python
def reparameterize(mu, log_var):
    std = torch.exp(0.5 * log_var)
    eps = torch.randn_like(std)
    return mu + std * eps
```

### KL Loss
```python
kl = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
```

### Reconstruction Loss (Bernoulli)
```python
recon = F.binary_cross_entropy_with_logits(x_recon, x, reduction='sum')
```

### Reconstruction Loss (Gaussian, fixed variance)
```python
recon = F.mse_loss(x_recon, x, reduction='sum') * 0.5
```

### Full Loss
```python
loss = (recon_loss + kl_loss) / batch_size
```

---

## Architecture Template

```
ENCODER: x â†’ [FC â†’ ReLU]Ã—n â†’ (Î¼, log ÏƒÂ²)
         â†“
REPARAM: z = Î¼ + Ïƒ âŠ™ Îµ
         â†“
DECODER: z â†’ [FC â†’ ReLU]Ã—n â†’ xÌ‚ (logits)
```

---

## Hyperparameter Guidelines

| Parameter | Typical Range | Notes |
|-----------|--------------|-------|
| Latent dim | 10-512 | 20-64 for MNIST |
| Hidden units | 256-1024 | Match data complexity |
| Learning rate | 1e-4 to 1e-3 | Start with 1e-3 |
| Batch size | 64-256 | 128 is common |
| Î² (KL weight) | 0.1-10 | 1 for standard VAE |

---

## Debugging Quick Reference

| Problem | First Fix |
|---------|-----------|
| KL â†’ 0 | KL annealing |
| NaN loss | Clamp log_var, lower LR |
| Blurry output | Lower Î², bigger decoder |
| Same reconstructions | Check encoder output variance |

### KL Annealing
```python
beta = min(1.0, epoch / warmup_epochs)
loss = recon + beta * kl
```

---

## Key Distributions

| Symbol | Distribution | VAE Role |
|--------|-------------|----------|
| p(z) | ð’©(0, I) | Prior |
| q_Ï†(z\|x) | ð’©(Î¼_Ï†(x), ÏƒÂ²_Ï†(x)) | Encoder |
| p_Î¸(x\|z) | Bernoulli or ð’© | Decoder |

---

## Variants Quick Guide

| Variant | Key Change | Use Case |
|---------|-----------|----------|
| Î²-VAE | Î² > 1 on KL | Disentanglement |
| CVAE | Condition on labels | Controlled generation |
| VQ-VAE | Discrete latents | Compression |
| IWAE | K samples, tighter bound | Better density |

---

## Evaluation Metrics

| Metric | Compute | Better |
|--------|---------|--------|
| ELBO | -loss | Higher |
| Recon | BCE or MSE | Lower |
| KL | Closed-form | Balanced |
| FID | pytorch-fid | Lower |

---

## Training Checklist

- [ ] Use log_var (not Ïƒ directly)
- [ ] BCE with logits (not after sigmoid)
- [ ] Sum over dims, mean over batch
- [ ] Set seeds for reproducibility
- [ ] Monitor KL per dimension
- [ ] Save checkpoints
- [ ] Use gradient clipping if unstable

---

## Generation vs Training

**Training:** Sample z from encoder q_Ï†(z|x)
```python
z = reparameterize(mu, log_var)  # Uses input x
```

**Generation:** Sample z from prior p(z)
```python
z = torch.randn(n_samples, latent_dim)  # No input needed
```

---

## Common Equations in Code Form

```python
# ELBO components
E_q[log p(x|z)] â‰ˆ -BCE(x, decoder(z))
D_KL = 0.5 * sum(muÂ² + exp(log_var) - log_var - 1)

# Loss (negative ELBO)
loss = BCE + KL

# Gaussian log-likelihood
log_p = -0.5 * (log(2Ï€) + log(ÏƒÂ²) + (x-Î¼)Â²/ÏƒÂ²)
```

---

**Remember:** VAE = Encoder + Reparameterize + Decoder + (Recon + KL) Loss


